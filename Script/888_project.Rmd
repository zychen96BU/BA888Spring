---
title: "888_project"
author: "BA888_Team3"
date: "4/3/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Install packages
```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(tidytext)
library(skimr)
library(dplyr)
library(purrr)
library(cluster)
library(factoextra)
library(corrplot)
library(arules)
library(arulesViz)
library(psych)
library(GPArotation)
library(leaflet)
library(tm)
library(wordcloud)
library(quanteda)
library(topicmodels)
library(ggplot2)
```

## load olist datasets
```{r}
customer <- read.csv('Data/olist_customers_dataset.csv')
order <- read.csv('Data/olist_orders_dataset.csv')
order_item <- read.csv('Data/olist_order_items_dataset.csv')
order_payment <- read.csv('Data/olist_order_payments_dataset.csv')
order_reviews <- read.csv('Data/olist_order_reviews_dataset.csv')
order_review_translated <- read.csv('Data/Translated_reviews - order_review_translated.csv')

product <- read.csv('Data/olist_products_dataset.csv')
location <- read.csv('Data/olist_geolocation_dataset.csv')
sellers <- read.csv('Data/olist_sellers_dataset.csv')
```

# load marketing funnel dataset
```{r}
library(tidyverse)
cd <- read.csv("Data/olist_closed_deals_dataset.csv")
mql <- read.csv("Data/olist_marketing_qualified_leads_dataset.csv")
marketing_funnel<- merge(cd, mql, by="mql_id")

location1 = location %>% group_by(geolocation_zip_code_prefix) %>% 
  summarize(mean_lat = mean(geolocation_lat),
            mean_long = mean(geolocation_lng))
```

## join all the dataset
```{r setup, include=FALSE}
p<- left_join(left_join(left_join(customer, order),order_item),product)
transaction <- left_join(p,order_payment)
transaction1 <- left_join(transaction,location1,
                          by = c("customer_zip_code_prefix"="geolocation_zip_code_prefix"))
transaction2 <- na.omit(left_join(transaction1,sellers))

transaction <- transaction2 %>%
  mutate(major_state = if_else(customer_state == c('SP','RJ','MG','BA','PA','PE'), 1, 0))
transaction_review<-left_join(transaction, order_reviews, by= c("order_id"="order_id"))

```

## Clean transaction date
```{r}
transaction$order_estimated_delivery_date <- as.POSIXct(transaction$order_estimated_delivery_date, format="%Y-%m-%d %H:%M:%S")
transaction$order_approved_at <- as.POSIXct(transaction$order_approved_at, format="%Y-%m-%d %H:%M:%S")




transaction$order_delivered_customer_date <- as.POSIXct(transaction$order_delivered_customer_date, format="%Y-%m-%d %H:%M:%S")
## using ceiling() to get the smallest intergers not less than the corresponding elements of x
transaction$deliverd_difftime <- ceiling(as.numeric(difftime(transaction$order_delivered_customer_date ,transaction$order_estimated_delivery_date), units = "days"))



transaction$delivered_days <- as.numeric(difftime(transaction$order_delivered_customer_date ,transaction$order_approved_at), units = "days")


transaction <- na.omit(transaction)

```

## Merge and create csv file
```{r}
final <- merge(marketing_funnel, transaction, by="seller_id")
translated_review_clean <- order_review_translated %>% 
  select(order_id,review_score,review_comments)

final_withreview <- merge(final,translated_review_clean, by="order_id")

final_withreview <- left_join(final,translated_review_clean, by="order_id")
write_csv(final_withreview,"final_dataset.csv")

useful_var<-c("price","deliverd_difftime","delivered_days",
              "payment_value","freight_value","product_category_name",
              "product_name_lenght","product_description_lenght","product_photos_qty"
              )

#colnames(final_withreview)
```


## Text Analysis

```{r}
## Text Analysis

reviews <- read_csv("Data/olist_order_reviews_dataset.csv")
library(tokenizers)
order_reviews = na.omit(reviews)
order_reviews$review_comment_message = tolower(order_reviews$review_comment_message)
order_reviews1 <- order_reviews %>%
  unnest_tokens(token, review_comment_message) 

stopwords::stopwords_getsources() 
stopwords::stopwords_getlanguages("misc") 
stopwords::stopwords_getlanguages("snowball") 
stopwords::stopwords_getlanguages("stopwords-iso") 
stopwords::stopwords_getlanguages("smart") 
stopwords::stopwords("portuguese")->porStopwords
porStopwords<-as.data.frame(porStopwords)
names(porStopwords)[1]<-"word"

order_reviews2 <- order_reviews1 %>%
  anti_join(porStopwords, by = c('token' = 'word')) 
order_reviews_sum <- order_reviews2 %>%
  group_by(token) %>%
  count(sort = T)

## Word Cloud Plot
wordcloud(words = order_reviews_sum$token,
          freq = order_reviews_sum$n, min.freq = 10, max.words = 50)

## LDA model
order_reviews_corpus <- corpus(order_reviews$review_comment_message) 
#order_reviews_corpus1 <- tm_map(order_reviews_corpus, removeWords, porStopwords)

summary(order_reviews_corpus, n = 20, showmeta = T) 
order_reviews_dfm <- dfm(order_reviews_corpus,remove_punct= T,remove = c(stopwords::stopwords("portuguese"),"Ã©"), remove_numbers= T, remove_symbols= T) %>%
  dfm_trim(min_termfreq = 2, max_docfreq = .5,
           docfreq_type = "prop") 
order_reviews_dtm <- convert(order_reviews_dfm, 'topicmodels')
order_reviews_lda <- LDA(order_reviews_dtm, k = 2, control = list(seed = 729))
terms(order_reviews_lda, 10) ->topicsPro
```

## linear
```{r}
# compute MSEs
fit_lm <- lm(f1, ef_train)
# MSE Train
yhat_train_lm <- predict(fit_lm)
mse_train_lm <- mean((y_train - yhat_train_lm)^2)
paste("Linear Regression Train MSE", mse_train_lm)
# MSE Test
yhat_test_lm <- predict(fit_lm, ef_test) 
mse_test_lm <- mean((y_test - yhat_test_lm)^2)
paste("Linear Regression Test MSE", mse_test_lm)
coef(fit_lm)
summary(fit_lm)
```

## set formula
```{r}

useful_var<-c("price","deliverd_difftime","delivered_days",
              "payment_value","freight_value","product_category_name",
              "product_name_lenght","product_description_lenght","product_photos_qty",
              "review_score")
final_withreview %>% 
  select(declared_monthly_revenue,price,freight_value,product_name_lenght:payment_sequential,payment_installments:mean_long,major_state:review_score)->prediction_dataset


# final_withreview %>% 
#   select(useful_var)->prediction_dataset



set.seed(1234)
prediction_dataset$train <- sample(c(0,1), nrow(prediction_dataset), replace = T, prob = c(.3, .7))
ef_test <- prediction_dataset %>% filter(train == 0)
ef_train <- prediction_dataset %>% filter(train == 1)



# data preparation
f1<-as.formula(paste(names(prediction_dataset)[19], paste(names(prediction_dataset)[1:18], collapse=" + "), sep=" ~ "))
x_train <- model.matrix(f1, ef_train)[, -1]
y_train <- ef_train$review_score
x_test <- model.matrix(f1, ef_test)[, -1]
y_test <- ef_test$review_score
```




### forward
```{r}
xnames <- colnames(ef_train)
xnames <- xnames[!xnames %in% c("train", "review_score")]
fit_fw <- lm(review_score ~ 1, data = ef_train)
yhat_train <- predict(fit_fw, ef_train)
yhat_test <- predict(fit_fw, ef_test)
mse_train <- mean((ef_train$review_score - yhat_train) ^ 2)
mse_test <- mean((ef_test$review_score - yhat_test) ^ 2)
xname <- "intercept"
log_fw <-
  tibble(
    xname = xname,
    model = paste0(deparse(fit_fw$call), collapse = ""),
    mse_train = mse_train,
    mse_test = mse_test
  )
###
while (length(xnames) > 0) {
  best_mse_train <- NA
  best_mse_test <- NA
  best_fit_fw <- NA
  best_xname <- NA
  # select the next best predictor
  for (xname in xnames) {
    # take a moment to examine and understand the following line
    fit_fw_tmp <- update(fit_fw, as.formula(paste0(". ~ . + ", xname)))
    # compute MSE train
    yhat_train_tmp <- predict(fit_fw_tmp, ef_train)
    mse_train_tmp <- mean((ef_train$review_score - yhat_train_tmp) ^ 2)
    # compute MSE test
    yhat_test_tmp <- predict(fit_fw_tmp, ef_test)
    mse_test_tmp <- mean((ef_test$review_score - yhat_test_tmp) ^ 2)
    # if this is the first predictor to be examined,
    # or if this predictors yields a lower MSE that the current
    # best, then store this predictor as the current best predictor
    if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
      best_xname <- xname
      best_fit_fw <- fit_fw_tmp
      best_mse_train <- mse_train_tmp
      best_mse_test <- mse_test_tmp
    }
  }
  log_fw <-
    log_fw %>% add_row(
      xname = best_xname,
      model = paste0(deparse(best_fit_fw$call), collapse = ""),
      mse_train = best_mse_train,
      mse_test = best_mse_test
    )
  # adopt the best model for the next iteration
  fit_fw <- best_fit_fw
  
  # remove the current best predictor from the list of predictors
  xnames <- xnames[xnames!=best_xname]
}

ggplot(log_fw, aes(seq_along(xname), mse_test)) +
  geom_point() +
  geom_line() +
  geom_point(aes(y=mse_train), color="blue") +
  geom_line(aes(y=mse_train), color="blue") +
  scale_x_continuous("Variables", labels = log_fw$xname, breaks = seq_along(log_fw$xname)) +
  scale_y_continuous("MSE test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(log_fw)
```

## backward
```{r}
xnames <- colnames(ef_train)
xnames <- xnames[!xnames %in% c("train","review_score")]
fit_bk <- lm(review_score ~ ., data = ef_train)
yhat_train_bk <- predict(fit_bk, ef_train)
yhat_test_bk <- predict(fit_bk, ef_test)
mse_train_bk <- mean((ef_train$review_score - yhat_train_bk) ^ 2)
mse_test_bk <- mean((ef_test$review_score - yhat_test_bk) ^ 2)
xname <- "intercept"
log_bk <-
  tibble(
    xname = xname,
    model = paste0(deparse(fit_bk$call), collapse = ""),
    mse_train_bk = mse_train_bk,
    mse_test_bk = mse_test_bk
  )
###
while (length(xnames) > 0) {
  best_mse_train_bk <- NA
  best_mse_test_bk <- NA
  best_fit_bk <- NA
  best_xname_bk <- NA
  # select the next best predictor
  for (xname in xnames) {
    # take a moment to examine and understand the following line
    fit_bk_tmp <- update(fit_bk, as.formula(paste0(". ~ . - ", xname)))
    # compute MSE train
    yhat_train_tmp_bk <- predict(fit_bk_tmp, ef_train)
    mse_train_tmp_bk <- mean((ef_train$review_score - yhat_train_tmp_bk) ^ 2)
    # compute MSE test
    yhat_test_tmp_bk <- predict(fit_bk_tmp, ef_test)
    mse_test_tmp_bk <- mean((ef_test$review_score - yhat_test_tmp_bk) ^ 2)
    # if this is the first predictor to be examined,
    # or if this predictors yields a lower MSE that the current
    # best, then store this predictor as the current best predictor
    if (is.na(best_mse_test_bk) | mse_test_tmp_bk > best_mse_test_bk) {
      best_xname_bk <- xname
      best_fit_bk <- fit_bk_tmp
      best_mse_train_bk <- mse_train_tmp_bk
      best_mse_test_bk <- mse_test_tmp_bk
    }
  }
  log_bk <-
    log_bk %>% add_row(
      xname = best_xname_bk,
      model = paste0(deparse(best_fit_bk$call), collapse = ""),
      mse_train_bk = best_mse_train_bk,
      mse_test_bk = best_mse_test_bk
    )
  # adopt the best model for the next iteration
  fit_bk <- best_fit_bk
  
  # remove the current best predictor from the list of predictors
  xnames <- xnames[xnames!=best_xname_bk]
}

ggplot(log_bk, aes(seq_along(xname), mse_test_bk)) +
  geom_point() +
  geom_line() +
  geom_point(aes(y=mse_train_bk), color="blue") +
  geom_line(aes(y=mse_train_bk), color="blue") +
  scale_x_continuous("Variables", labels = log_bk$xname, breaks = seq_along(log_bk$xname)) +
  scale_y_continuous("MSE test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(log_bk)
```

## Model #4: Ridge Regression
```{r,warning=FALSE}
##lambda
est_r <- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)
#est_r <- glmnet(x_train, y_train, alpha = 0, nlambda = 100)
y_train_hat_r <- predict(est_r,x_train,s = est_r$lambda.min)
mse_train_r <- mean((y_train - y_train_hat_r)^2)
#y_train_hat

y_test_hat_r <- predict(est_r, x_test,s = est_r$lambda.min)
mse_test_r <- mean((y_test - y_test_hat_r)^2)
##y_test_hat
##lm<-function(x,y) {
  #mean((x-y)^2)
##}
# write code to create a vector that contains 100 MSE estimates for the train data
#use apply() to use the funciton created
##mse_train_r<-apply(y_train_hat_r,2,lm,y=y_train)
# write code to create a vector that contains 100 MSE estimates for the test data
#use apply() to use the funciton created
##mse_test_r<-apply(y_test_hat_r,2,lm,y=y_test)
lambda_min_mse_train_r <- mse_train_r[which.min(mse_train_r)]
lambda_min_mse_test_r <- mse_test_r[which.min(mse_test_r)]
# create a tibble of train MSEs and lambdas
##ef_mse_r <- tibble(
  ##lambda = est_r$lambda,
  ##mse = mse_train_r,
  ##dataset = "Train"
##)
##ef_mse_r<- rbind(ef_mse_r, tibble(
  ##lambda = est_r$lambda,
  ##mse = mse_test_r,
  ##dataset = "Test"
##))
# Use the rbind command to combine dd_mse_train
# and dd_mse_test into a single data frame

#ef_mse_r %>% 
  #ggplot(aes(x=lambda,y=mse,color=dataset))+
  #geom_line()+
  #reverse x scale
  #scale_x_reverse()+
  #geom_point(data =filter(ef_mse_r,dataset=="Train"),aes(x=lambda[which.min(mse)],y=min(mse)))+
  #geom_point(data =filter(ef_mse_r,dataset=="Test"),aes(x=lambda[which.min(mse)],y=min(mse))) 

paste("Ridge Regression Train MSE",lambda_min_mse_train_r)
paste("Ridge Regression Test MSE",lambda_min_mse_test_r)

coef(est_r,lambda_min_mse_test_r)
```
## select the significance variables from selections
```{r}
var<-c("price","product_weight_g","payment_value","delivered_days",
       "freight_value","product_width_cm","payment_sequential","payment_installments")
prediction_dataset %>% 
  select(var,review_score,train)->pred1




ef_test <- pred1 %>% filter(train == 0)
ef_train <- pred1 %>% filter(train == 1)



# data preparation
f1<-as.formula(paste(names(pred1)[9], paste(names(pred1)[1:8], collapse=" + "), sep=" ~ "))
x_train <- model.matrix(f1, ef_train)[, -1]
y_train <- ef_train$review_score
x_test <- model.matrix(f1, ef_test)[, -1]
y_test <- ef_test$review_score


```

##  Ridge Regression
```{r,warning=FALSE}
##lambda
est_r <- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)
#est_r <- glmnet(x_train, y_train, alpha = 0, nlambda = 100)
y_train_hat_r <- predict(est_r,x_train,s = est_r$lambda.min)
mse_train_r <- mean((y_train - y_train_hat_r)^2)
#y_train_hat

y_test_hat_r <- predict(est_r, x_test,s = est_r$lambda.min)
mse_test_r <- mean((y_test - y_test_hat_r)^2)
##y_test_hat
##lm<-function(x,y) {
  #mean((x-y)^2)
##}
# write code to create a vector that contains 100 MSE estimates for the train data
#use apply() to use the funciton created
##mse_train_r<-apply(y_train_hat_r,2,lm,y=y_train)
# write code to create a vector that contains 100 MSE estimates for the test data
#use apply() to use the funciton created
##mse_test_r<-apply(y_test_hat_r,2,lm,y=y_test)
lambda_min_mse_train_r <- mse_train_r[which.min(mse_train_r)]
lambda_min_mse_test_r <- mse_test_r[which.min(mse_test_r)]
# create a tibble of train MSEs and lambdas
##ef_mse_r <- tibble(
  ##lambda = est_r$lambda,
  ##mse = mse_train_r,
  ##dataset = "Train"
##)
##ef_mse_r<- rbind(ef_mse_r, tibble(
  ##lambda = est_r$lambda,
  ##mse = mse_test_r,
  ##dataset = "Test"
##))
# Use the rbind command to combine dd_mse_train
# and dd_mse_test into a single data frame

#ef_mse_r %>% 
  #ggplot(aes(x=lambda,y=mse,color=dataset))+
  #geom_line()+
  #reverse x scale
  #scale_x_reverse()+
  #geom_point(data =filter(ef_mse_r,dataset=="Train"),aes(x=lambda[which.min(mse)],y=min(mse)))+
  #geom_point(data =filter(ef_mse_r,dataset=="Test"),aes(x=lambda[which.min(mse)],y=min(mse))) 

paste("Ridge Regression Train MSE",lambda_min_mse_train_r)
paste("Ridge Regression Test MSE",lambda_min_mse_test_r)

coef(est_r,lambda_min_mse_test_r)
```

## Lasso Regression
```{r,warning=FALSE}
##lambda
library(glmnet)
est_l <- glmnet(x_train, y_train, alpha = 1, nlambda = 100)
y_train_hat_l <- predict(est_l,x_train)
#y_train_hat
y_test_hat_l <- predict(est_l, x_test)
#y_test_hat
lm<-function(x,y) {
  mean((x-y)^2)
}
# write code to create a vector that contains 100 MSE estimates for the train data
#use apply() to use the funciton created
mse_train_l<-apply(y_train_hat_l,2,lm,y=y_train)
# write code to create a vector that contains 100 MSE estimates for the test data
#use apply() to use the funciton created
mse_test_l<-apply(y_test_hat_l,2,lm,y=y_test)
lambda_min_mse_train_l <- mse_train_l[which.min(mse_train_l)]
lambda_min_mse_test_l <- mse_test_l[which.min(mse_test_l)]
# create a tibble of train MSEs and lambdas
ef_mse_l <- tibble(
  lambda = est_l$lambda,
  mse = mse_train_l,
  dataset = "Train"
)
ef_mse_l<- rbind(ef_mse_l, tibble(
  lambda = est_l$lambda,
  mse = mse_test_l,
  dataset = "Test"
))
# Use the rbind command to combine dd_mse_train
# and dd_mse_test into a single data frame

ef_mse_l %>% 
  ggplot(aes(x=lambda,y=mse,color=dataset))+
  geom_line()+
  #reverse x scale
  scale_x_reverse()+
  geom_point(data =filter(ef_mse_l,dataset=="Train"),aes(x=lambda[which.min(mse)],y=min(mse)))+
  geom_point(data =filter(ef_mse_l,dataset=="Test"),aes(x=lambda[which.min(mse)],y=min(mse))) 
paste("Lasso Regression Train MSE",lambda_min_mse_train_l)
paste("Lasso Regression Test MSE",lambda_min_mse_test_l)

coef(est_l,lambda_min_mse_test_l)
```

## randromForest
```{r}
library(randomForest)
fit_rf <- randomForest(f1, ef_train, ntree = 500, do.trace=F)
yhat_rf_train <- predict(fit_rf, ef_train)
mse_rf_train <- mean((yhat_rf_train - y_train) ^2)
yhat_rf_test <- predict(fit_rf, ef_test)
mse_rf_test <- mean((yhat_rf_test - y_test) ^2)
as.data.frame(importance(fit_rf))->importance
cbind(importance,rownames(importance))->importance
rownames(importance)<-seq(1,8,1)
importance[order(-importance$IncNodePurity),]<-importance

varImpPlot(fit_rf)
paste("Random Forest Train MSE",mse_rf_train)
paste("Random Forest Test MSE",mse_rf_test)
```
### importance
```{r}
as.data.frame(importance(fit_rf))->importance
cbind(importance,rownames(importance))->importance
rownames(importance)<-seq(1,8,1)
importance[order(-importance$IncNodePurity),]<-importance
importance[order(-importance$IncNodePurity),]
```

## boosting
```{r}
library(gbm)
library(glmnet)
fit_btree <- gbm(f1, data = ef_train, distribution = "gaussian",
                 n.trees = 1000, interaction.depth = 2, shrinkage = 0.1)
relative.influence(fit_btree)
yhat_btree_train <- predict(fit_btree, ef_train, n.trees = 100)
mse_btree_train <- mean((yhat_btree_train - y_train) ^ 2)
yhat_btree_test <- predict(fit_btree, ef_test, n.trees = 100)
mse_btree_test <- mean((yhat_btree_test - y_test) ^ 2)

paste("Boosting Trees Train MSE",mse_btree_train)
paste("Boosting Trees Test MSE",mse_btree_test)
```